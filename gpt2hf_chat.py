from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import PyPDFLoader
from langchain.llms import HuggingFacePipeline
import os

os.environ["OPENAI_API_KEY"] = ""  # your key
loader = DirectoryLoader('./articles', glob="*.pdf", loader_cls=PyPDFLoader)

documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# 1000ê¸€ìì”© ë¶„í• í•˜ê¸°
# ëŠê¸°ëŠ”ê²ƒì„ ë°©ì§€í•˜ì§€ ìœ„í•´ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ 200ìë¡œ ì œí•œí•¨.
texts = text_splitter.split_documents(documents)

persist_directory = 'db'  # 'db' ë””ë ‰í† ë¦¬ì— ì €ì¥í•¨.

embedding = OpenAIEmbeddings()
# openaiì˜ embeddingì„ ì‚¬ìš©í•¨

vectordb = Chroma.from_documents(
    documents=texts,
    embedding=embedding,
    persist_directory=persist_directory)

# ì´ˆê¸°í™” í•˜ëŠ” í•˜ëŠ” ê³¼ì •
vectordb.persist()
vectordb = None
persist_directory = 'chroma_db'

vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding)

retriever = vectordb.as_retriever()

# model_id = 'maywell/Synatra-42dot-1.3B'
model_id = 'zomd/AISquare-Instruct-yi-ko-6b-v0.9.30'
os.environ['HF_HOME'] = r'./models'

llm = HuggingFacePipeline.from_model_id(
    model_id=model_id,
    device=0,  # -1: CPU(default), 0ë²ˆ ë¶€í„°ëŠ” CUDA ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ì§€ì •ì‹œ GPU ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ 
    task="text-generation",  # í…ìŠ¤íŠ¸ ìƒì„±
    model_kwargs={"temperature": 0.2,
                  "max_length": 2000},
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True)


def process_llm_response(llm_response):
    return llm_response['result']
    # print('\n\nSources:')
    # for source in llm_response["source_documents"]:
    #     print(source.metadata['source'])


## streamlit íŒŒíŠ¸
import streamlit as st
from streamlit_chat import message

st.header("ğŸ¤–Farming ChatBot")

if 'generated' not in st.session_state:
    st.session_state['generated'] = []

if 'past' not in st.session_state:
    st.session_state['past'] = []

form_submitted = False
with st.form('form', clear_on_submit=True):
    user_input = st.text_input('You: ', '', key='input')
    submitted = st.form_submit_button('Send')

if submitted and user_input:
    st.session_state.past.append(user_input)
    llm_response = qa_chain(user_input)

    output = {"generated_text": process_llm_response(llm_response)}

    st.session_state.generated.append(output["generated_text"])

if st.session_state['generated']:
    # doc = st.session_state.get('doc', Document())
    for i in range(len(st.session_state['generated']) - 1, -1, -1):
        message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')
        message(st.session_state["generated"][i], key=str(i))
        # doc.add_paragraph(f'You: {st.session_state["past"][i]}', style='Heading2')
        # doc.add_paragraph(f'ChatBot: {st.session_state["generated"][i]}')
    # st.session_state['doc'] = doc

    # # Word ë¬¸ì„œ ì €ì¥
    # script_dir = os.path.dirname(os.path.abspath(__file__))
    # file_path = os.path.join(script_dir, "chatbot_conversation.docx")
    # doc.save(file_path)

    # # ì €ì¥ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
    # st.markdown(f'[Download ChatBot Conversation]({file_path})', unsafe_allow_html=True)
