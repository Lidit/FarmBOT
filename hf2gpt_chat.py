import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import PyPDFLoader

os.environ["OPENAI_API_KEY"] = ""  # your key

loader = DirectoryLoader('./articles', glob="*.pdf", loader_cls=PyPDFLoader)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# 1000ê¸€ìì”© ë¶„í• í•˜ê¸°
# ëŠê¸°ëŠ”ê²ƒì„ ë°©ì§€í•˜ì§€ ìœ„í•´ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì„ 200ìë¡œ ì œí•œí•¨.
texts = text_splitter.split_documents(documents)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ

embeddings = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask", encode_kwargs={'normalize_embeddings': True}
)

# Chroma DBì— ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•˜ê¸°
vectordb = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")

# ì´ˆê¸°í™” í•˜ëŠ” í•˜ëŠ” ê³¼ì •
vectordb.persist()
vectordb = None
persist_directory = 'chroma_db'

vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings)

# retriever ìƒì„±
retriever = vectordb.as_retriever()

# retirieval QA chain init
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True)


def process_llm_response(llm_response):
    print(llm_response['result'])
    print('\n\nSources:')
    for source in llm_response["source_documents"]:
        print(source.metadata['source'])


## streamlit íŒŒíŠ¸
import streamlit as st
from streamlit_chat import message

st.header("ğŸ¤–Farming ChatBot")

if 'generated' not in st.session_state:
    st.session_state['generated'] = []

if 'past' not in st.session_state:
    st.session_state['past'] = []

form_submitted = False
with st.form('form', clear_on_submit=True):
    user_input = st.text_input('You: ', '', key='input')
    submitted = st.form_submit_button('Send')

if submitted and user_input:
    st.session_state.past.append(user_input)
    llm_response = qa_chain(user_input)

    output = {"generated_text": process_llm_response(llm_response)}

    st.session_state.generated.append(output["generated_text"])

if st.session_state['generated']:
    # doc = st.session_state.get('doc', Document())
    for i in range(len(st.session_state['generated']) - 1, -1, -1):
        message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')
        message(st.session_state["generated"][i], key=str(i))
        # doc.add_paragraph(f'You: {st.session_state["past"][i]}', style='Heading2')
        # doc.add_paragraph(f'ChatBot: {st.session_state["generated"][i]}')
    # st.session_state['doc'] = doc

    # # Word ë¬¸ì„œ ì €ì¥
    # script_dir = os.path.dirname(os.path.abspath(__file__))
    # file_path = os.path.join(script_dir, "chatbot_conversation.docx")
    # doc.save(file_path)

    # # ì €ì¥ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
    # st.markdown(f'[Download ChatBot Conversation]({file_path})', unsafe_allow_html=True)
